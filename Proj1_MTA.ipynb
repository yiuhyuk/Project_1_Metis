{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nArchive of old code:\\n\\nprimary_df = mta_turn_sorted.iloc[1:].copy()\\nless_df = mta_turn_sorted.iloc[:-1]\\nprimary_df['Entries_diff'] = primary_df['ENTRIES'] - less_df['ENTRIES'].values\\nprimary_df['Exits_diff'] = primary_df['EXITS'] - less_df['EXITS'].values\\n\\ndef diff_turnstiles(df):\\n    df_sorted = df.sort_values(by=['DATE','TIME']).copy()\\n    df_sorted['entries_diff'] = df_sorted['ENTRIES'].diff()\\n    df_sorted['exits_diff'] = df_sorted['EXITS'].diff()\\n    return df_sorted\\n\\nmaster_df = pd.DataFrame()\\nfor ca in mta_turn_190330['C/A'].unique():\\n    for unit in mta_turn_190330['UNIT'].unique():\\n        for scp in mta_turn_190330['SCP'].unique():\\n            mta_turn_filtered = pd.DataFrame()\\n            mta_turn_filtered = mta_turn_190330[(mta_turn_190330['C/A']==ca) & (mta_turn_190330.UNIT==unit) & (mta_turn_190330.SCP==scp)]\\n            pd.concat([master_df, diff_turnstiles(mta_turn_filtered)])\\n\\nstation_df = mta_turn_190330.groupby(by=['STATION','DATE','TIME']).sum()\\nstation_df['entries_diff'] = station_df['ENTRIES'].diff()\\nstation_df\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Archive of old code:\n",
    "\n",
    "primary_df = mta_turn_sorted.iloc[1:].copy()\n",
    "less_df = mta_turn_sorted.iloc[:-1]\n",
    "primary_df['Entries_diff'] = primary_df['ENTRIES'] - less_df['ENTRIES'].values\n",
    "primary_df['Exits_diff'] = primary_df['EXITS'] - less_df['EXITS'].values\n",
    "\n",
    "def diff_turnstiles(df):\n",
    "    df_sorted = df.sort_values(by=['DATE','TIME']).copy()\n",
    "    df_sorted['entries_diff'] = df_sorted['ENTRIES'].diff()\n",
    "    df_sorted['exits_diff'] = df_sorted['EXITS'].diff()\n",
    "    return df_sorted\n",
    "\n",
    "master_df = pd.DataFrame()\n",
    "for ca in mta_turn_190330['C/A'].unique():\n",
    "    for unit in mta_turn_190330['UNIT'].unique():\n",
    "        for scp in mta_turn_190330['SCP'].unique():\n",
    "            mta_turn_filtered = pd.DataFrame()\n",
    "            mta_turn_filtered = mta_turn_190330[(mta_turn_190330['C/A']==ca) & (mta_turn_190330.UNIT==unit) & (mta_turn_190330.SCP==scp)]\n",
    "            pd.concat([master_df, diff_turnstiles(mta_turn_filtered)])\n",
    "\n",
    "station_df = mta_turn_190330.groupby(by=['STATION','DATE','TIME']).sum()\n",
    "station_df['entries_diff'] = station_df['ENTRIES'].diff()\n",
    "station_df\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mta_turn_190330 = pd.read_csv('http://web.mta.info/developers/data/nyct/turnstile/turnstile_190330.txt')\n",
    "#mta_turn_190323 = pd.read_csv('http://web.mta.info/developers/data/nyct/turnstile/turnstile_190323.txt')\n",
    "#mta_turn_190316 = pd.read_csv('http://web.mta.info/developers/data/nyct/turnstile/turnstile_190316.txt')\n",
    "#mta_turn_190309 = pd.read_csv('http://web.mta.info/developers/data/nyct/turnstile/turnstile_190309.txt')\n",
    "#mta_turn_190302 = pd.read_csv('http://web.mta.info/developers/data/nyct/turnstile/turnstile_190302.txt')\n",
    "mta_turn_all = pd.read_csv('turnstile_data.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mta_turn_all.rename(columns={mta_turn_all.columns.values[-1]: 'EXITS'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3548154, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mta_turn_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Remove duplicate headers and pickle the file\n",
    "'''\n",
    "\n",
    "def remove_headers(df, header1, load_file):\n",
    "    if load_file == False:\n",
    "        drop_rows = [index for index, val in df.iterrows() if val[header1]==header1]\n",
    "        df.drop(drop_rows, inplace=True)\n",
    "        # open a file, where you ant to store the data\n",
    "        file = open('raw_clean', 'wb')\n",
    "        # dump information to that file\n",
    "        pickle.dump(df, file)\n",
    "        # close the file\n",
    "        file.close()\n",
    "    else:\n",
    "        # open a file, where you stored the pickled data\n",
    "        file = open('raw_clean', 'rb')\n",
    "        # dump information to that file\n",
    "        df = pickle.load(file)\n",
    "        # close the file\n",
    "        file.close()\n",
    "    return df\n",
    "\n",
    "\n",
    "'''\n",
    "Make a dict that maps times to an integer mapping of time of the day (or standardized time of the day).\n",
    "We can remove the non-whole number times or we will end up double counting some entries and exits.\n",
    "'''\n",
    "\n",
    "def get_day_of_week(date):\n",
    "    return datetime.datetime.strptime(date, '%m/%d/%Y').weekday()\n",
    "\n",
    "def get_month(date):\n",
    "    return datetime.datetime.strptime(date, '%m/%d/%Y').month\n",
    "\n",
    "def get_day(date):\n",
    "    return datetime.datetime.strptime(date, '%m/%d/%Y').day\n",
    "\n",
    "def get_week(date):\n",
    "    return datetime.datetime.strptime(date, '%m/%d/%Y').isocalendar()[1]\n",
    "\n",
    "def standard_time(time_str_in):\n",
    "    time_time = time.strptime(time_str_in, '%H:%M:%S')\n",
    "    if (time_time.tm_min != 0) or (time_time.tm_sec != 0):\n",
    "        return np.nan\n",
    "    else:\n",
    "        # Logic for standardizing the times to be generally consistent and comparable\n",
    "        time1 = time.strptime('00:00:00', '%H:%M:%S')\n",
    "        time2 = time.strptime('04:00:00', '%H:%M:%S')\n",
    "        time3 = time.strptime('08:00:00', '%H:%M:%S')\n",
    "        time4 = time.strptime('12:00:00', '%H:%M:%S')\n",
    "        time5 = time.strptime('16:00:00', '%H:%M:%S')\n",
    "        time6 = time.strptime('20:00:00', '%H:%M:%S')\n",
    "        if (time_time >= time1) and (time_time < time2):\n",
    "            time_str_out = '02:00:00'\n",
    "        elif (time_time >= time2) and (time_time < time3):\n",
    "            time_str_out = '06:00:00'\n",
    "        elif (time_time >= time3) and (time_time < time4):\n",
    "            time_str_out = '10:00:00'\n",
    "        elif (time_time >= time4) and (time_time < time5):\n",
    "            time_str_out = '14:00:00'\n",
    "        elif (time_time >= time5) and (time_time < time6):\n",
    "            time_str_out = '18:00:00'\n",
    "        else:\n",
    "            time_str_out = '22:00:00'\n",
    "        return time_str_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3548137, 11)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turn_df = remove_headers(mta_turn_all, 'C/A', load_file=False)\n",
    "turn_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Cleaning the data\n",
    "'''\n",
    "turn_df['entries_int'] = pd.to_numeric(turn_df['ENTRIES'])\n",
    "turn_df['exits_int'] = pd.to_numeric(turn_df['EXITS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "turn_df['standard_time'] = turn_df['TIME'].apply(standard_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_clean = True\n",
    "\n",
    "if save_clean:\n",
    "    # open a file, where you want to store the data\n",
    "    file = open('raw_mod', 'wb')\n",
    "    # dump information to that file\n",
    "    pickle.dump(turn_df, file)\n",
    "    # close the file\n",
    "    file.close()\n",
    "else:\n",
    "    # open a file, where you stored the pickled data\n",
    "    file = open('raw_mod', 'rb')\n",
    "    # dump information to that file\n",
    "    turn_df = pickle.load(file)\n",
    "    # close the file\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "turn_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df = turn_df.groupby(by=['STATION','DATE','standard_time']).sum()\n",
    "station_df['entries_diff'] = station_df['entries_int'].diff()\n",
    "station_df['exits_diff'] = station_df['exits_int'].diff()\n",
    "station_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NaN out the rows where the station is changing as those values do not make sense\n",
    "'''\n",
    "station_chg_index = [i+1 for i, val in enumerate(station_df['STATION'][1:]) if val != station_df['STATION'][i]]\n",
    "for i in station_chg_index:\n",
    "    station_df.loc[i, 'entries_diff'] = np.nan\n",
    "    station_df.loc[i, 'exits_diff'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14:00:00    126\n",
       "06:00:00    126\n",
       "22:00:00    126\n",
       "18:00:00    126\n",
       "02:00:00    126\n",
       "10:00:00    126\n",
       "Name: standard_time, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station_df[station_df['STATION']=='103 ST']['standard_time'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df['day_of_week'] = station_df['DATE'].apply(get_day_of_week)\n",
    "station_df['month'] = station_df['DATE'].apply(get_month)\n",
    "station_df['day'] = station_df['DATE'].apply(get_day)\n",
    "station_df['week'] = station_df['DATE'].apply(get_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nan that result from station transition\n",
    "station_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     STATION        DATE standard_time  entries_int  exits_int  entries_diff  \\\n",
      "1470  103 ST  06/23/2018      06:00:00    152333107  598885745         257.0   \n",
      "1471  103 ST  06/23/2018      10:00:00    151170646  598432802    -1162461.0   \n",
      "1472  103 ST  06/23/2018      14:00:00    152340924  598890727     1170278.0   \n",
      "\n",
      "      exits_diff  day_of_week  month  day  week  \n",
      "1470       608.0            5      6   23    25  \n",
      "1471   -452943.0            5      6   23    25  \n",
      "1472    457925.0            5      6   23    25  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1162461"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_df = station_df[station_df['STATION']=='103 ST']\n",
    "error_times = debug_df[debug_df['DATE']=='06/23/2018'].groupby(by='standard_time').sum()[['entries_diff','exits_diff']]\n",
    "\n",
    "error_df = debug_df[(debug_df['DATE']=='06/23/2018') \n",
    "                    & ((debug_df['standard_time']=='10:00:00') \n",
    "                       | (debug_df['standard_time']=='06:00:00')\n",
    "                       | (debug_df['standard_time']=='14:00:00'))]\n",
    "print(error_df)\n",
    "error_df['entries_int'].iloc[1] - error_df['entries_int'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1946 rows dropped from df out of 265790 total\n",
      "318 rows dropped from df out of 263845 total\n"
     ]
    }
   ],
   "source": [
    "# Remove negatives and the row after as well (because there is usually a reversal)\n",
    "\n",
    "def clear_outliers(df, field):\n",
    "    original_len = df.shape[0]\n",
    "    clear_list = list(df[df[field] < 0].index)\n",
    "    clear_list_plus1 = [i+1 for i in clear_list]\n",
    "    clear_list.append(clear_list_plus1)\n",
    "    for i in clear_list:\n",
    "        if i in list(df.index):\n",
    "            df.loc[i, field] = np.nan\n",
    "    df.dropna(inplace=True)\n",
    "    print(str(len(clear_list)) + ' rows dropped from df out of ' + str(original_len) + ' total')\n",
    "    return df, len(clear_list)\n",
    "    \n",
    "station_df, rows_dropped_entries = clear_outliers(station_df, 'entries_diff')\n",
    "station_df, rows_dropped_entries = clear_outliers(station_df, 'exits_diff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df['total_traffic'] = station_df['entries_diff'] + station_df['exits_diff']\n",
    "grp_station_weekday = station_df.groupby(by=['STATION','day_of_week']).mean()['total_traffic']\n",
    "grp_station = station_df.groupby(by=['STATION']).mean()['total_traffic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "STATION\n",
       "42 ST-PORT AUTH    2.799034e+07\n",
       "104 ST             2.650511e+07\n",
       "3 AV-149 ST        2.450819e+07\n",
       "72 ST              2.347603e+07\n",
       "125 ST             2.298854e+07\n",
       "CANAL ST           1.891696e+07\n",
       "23 ST              1.715238e+07\n",
       "18 AV              1.436272e+07\n",
       "CHAMBERS ST        1.375036e+07\n",
       "34 ST-HERALD SQ    1.243992e+07\n",
       "14 ST              1.224272e+07\n",
       "86 ST              1.202819e+07\n",
       "KINGS HWY          1.185130e+07\n",
       "BROOKLYN BRIDGE    1.125392e+07\n",
       "FRANKLIN AV        1.057534e+07\n",
       "59 ST              1.046154e+07\n",
       "FULTON ST          1.014839e+07\n",
       "190 ST             1.011854e+07\n",
       "1 AV               1.004532e+07\n",
       "34 ST-PENN STA     9.956466e+06\n",
       "COURT SQ           9.423197e+06\n",
       "59 ST COLUMBUS     8.559343e+06\n",
       "BAY PKWY           8.070874e+06\n",
       "183 ST             7.517503e+06\n",
       "225 ST             7.020078e+06\n",
       "FLATBUSH AV-B.C    6.861365e+06\n",
       "47-50 STS ROCK     5.894355e+06\n",
       "6 AV               5.741863e+06\n",
       "GRD CNTRL-42 ST    5.218704e+06\n",
       "LEXINGTON AV/63    5.036421e+06\n",
       "                       ...     \n",
       "75 ST-ELDERTS      8.707556e+02\n",
       "138/GRAND CONC     8.698198e+02\n",
       "NECK RD            8.543873e+02\n",
       "CENTRAL AV         8.463924e+02\n",
       "163 ST-AMSTERDM    7.632959e+02\n",
       "SHEPHERD AV        7.225656e+02\n",
       "ZEREGA AV          7.158342e+02\n",
       "BURKE AV           6.732467e+02\n",
       "238 ST             6.406096e+02\n",
       "BEACH 67 ST        6.348501e+02\n",
       "HEWES ST           6.203351e+02\n",
       "JUNIUS ST          5.849164e+02\n",
       "21 ST              5.754403e+02\n",
       "88 ST              5.710517e+02\n",
       "AQUEDUCT N.COND    5.041733e+02\n",
       "55 ST              4.553992e+02\n",
       "215 ST             4.349588e+02\n",
       "ATLANTIC AV        3.963382e+02\n",
       "CYPRESS HILLS      3.365623e+02\n",
       "E 143/ST MARY'S    3.075854e+02\n",
       "BEACH 36 ST        2.849483e+02\n",
       "BEACH 44 ST        2.253825e+02\n",
       "AVENUE P           2.063971e+02\n",
       "ROCKAWAY PARK B    2.004164e+02\n",
       "BROAD CHANNEL      1.346141e+02\n",
       "HARRISON           1.250000e+02\n",
       "BEACH 98 ST        1.208424e+02\n",
       "TOMPKINSVILLE      1.192570e+02\n",
       "ORCHARD BEACH      8.952846e+01\n",
       "BEACH 105 ST       7.304636e+01\n",
       "Name: total_traffic, Length: 361, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp_station.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_clean = True\n",
    "\n",
    "if save_clean:\n",
    "    # open a file, where you want to store the data\n",
    "    file = open('final', 'wb')\n",
    "    # dump information to that file\n",
    "    pickle.dump(station_df, file)\n",
    "    # close the file\n",
    "    file.close()\n",
    "else:\n",
    "    # open a file, where you stored the pickled data\n",
    "    file = open('final', 'rb')\n",
    "    # dump information to that file\n",
    "    station_df = pickle.load(file)\n",
    "    # close the file\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
